---
layout: default
title: "Blackbox and Approximate Neural Inference"
---
# Blackbox and Approximate (Variational) Neural Inference 
For quite sometime now I've been working on neural inference methods that have become very popular recently. There is an abundance of resources on these methods and that is precisely why I decided to write a considerably terse post about the most prominent of such techniques. Since this is only an attempt to summarize the vast body of work being done in this field, I will try to provide links to more detailed (read: much better than mine) posts and paper for each of the methods. As mentioned earlier, this is only an attempt to summarize this fairly sofesticated field, please correct me via comments if you find any mistakes in my description, of which I can promise there will be plenty to fix :) Lastly before starting, this is more of a *dynamic* post, meaning that I will kepp updating the entries and adding details as I get time.

With that lets delve staright into our main topic, __Approximate Neural Inference__ or as we will refer to it throughout this text, __NI__. Simply put, varitional inference (__VI__) is a deterministic method of carrying out approximate inference in probabilistic models when the posterior distribution over the variables of interest (or the latent state) is intractable. In order to do so, VI uses a tractable family of distributions to approximate the intractable posterior in an optimization procedure that usually minimizes the negative log-likelihood of the data that the model is trying to fit. Consider the following example,

Let $$p_\Theta(x)=\int_zp(x|z,\alpha)p(z|\beta)dz$$ be the likelihood of our data $$x$$ under our model parametrized by $$\Theta=\{\alpha,\beta\}$$ where $$z$$ (continuous or discrete in which case $$\int$$ can be replaced by $$\sum$$) is a latent variable. Assuming, the posterior distribution $$p_\theta(z|x)$$ is intractable, mean-field VI (which is a type of VI) approximates this posterior by first introducing variational parameters $$\phi$$ for each $$z$$ and then aprroximate the true posterior $$p_\theta(z|x)$$ by the variational posterior $$q_\phi(z|x)$$. In practice, instead of directly optimizing the negative log-likelihood to fit the posterior, VI methods use a simpler to optimize lowerbound on it (popularly referred to as the __ELBO__ or the evidence lowerbound). Traditional VI methods make use of *conjugate* priors over the latent variables to derive closed form `EM-like` updates for optimization, which accounts for their fast speed of inference. But this is also the reason for their limited applicability and/or accuracy. Not all models can leverage the convinience of conjugate priors. Another drawback is the need of repeated derivations for even minor changes in initial assumptions. __NI__ can be thought of as an alternative form or approximate varitational inference that has a certain *black-box* characteristic to it and therefore allows for carrying out approximate inference without the need to derive update equations even in the models that do not have conjugate priors. 


